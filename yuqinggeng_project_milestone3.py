# -*- coding: utf-8 -*-
"""YuqingGeng_Project_Milestone3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKb5KuO32QV3-QlAhtVKFdoaufz4ZXmV

# 1. Problem Description
Low latency trading is now days widely applied in hedge firms, investment banks, etc. Strategies are mostly focusing on short term market changes, especially for day trading. Thus being able to balance the model run-time with accuracy rate in predicting market movements is the key for profiting. Since market data is non-stationary, the model required is always complex.
Traditionally, deep artificial neural network has been applied in generating trading algorithms. With its non-linear activation functions, ANN can satisfy the accuracy requirement to a certain degree [1]. However, it’s not the fastest way to process market data.

We'll be using BindsNet to test SNN's ability in accessing stock market data and predict the optimal protfoloi. Our goal is to be able to predict how much each stock should weight in our protfolio by matching the market pattern with the historical data, and use pattern recognization, discover the optimal weight distribution.
"""

!pip install bindsnet

import os
import torch
import argparse
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm

from time import time as t

from bindsnet import ROOT_DIR
from bindsnet.datasets import DataLoader
from bindsnet.encoding import PoissonEncoder
from bindsnet.models import DiehlAndCook2015
from bindsnet.network.monitors import Monitor
from bindsnet.utils import get_square_weights, get_square_assignments
from bindsnet.evaluation import all_activity, proportion_weighting, assign_labels
from bindsnet.analysis.plotting import (
    plot_input,
    plot_spikes,
    plot_weights,
    plot_assignments,
    plot_performance,
    plot_voltages,
)
from torch.utils.data import Dataset
from torchvision import transforms, utils

"""#2. Model Generation 
## 2.1 load data

The data we're using are from UChicago Midwest Trading Competition, which is not actual stock market data. Those data are geenrated according to unknown fundmental factors as well as certain unexpected market movements. 

The data includes 10 years' daily stock price for ten stocks, 10 years' market index (we can view it as S&P500, for example), and annual risk free rate calculated every month.
"""

from google.colab import drive
drive.mount('/content/drive')

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My\ Drive/CSC691\ Project

stock_price_csv = ('stock_prices_train.csv')
risk_free_rate_csv = ('risk_free_train.csv')

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")

plt.ion()   # interactive mode

"""#3: Labeling Data
Unlike MNIST dataset, stock data doesn't come with a label. However, we can modify the SNN for MNIST to generate a network which tells us how to make investment decisions. In order to do so, we need to have a standard of "what is a good investment decision". 

There are two things we care about: profit, and risk. Profit is measured by $\alpha$, the active return on an investment; risk is measured by $\beta$, the volatility. We want to assign each stock in our protfolio a certain weights, and if we multiply stock price with the corresponding weights on a given day, we shall be able to tell whether investment decision is prudent by looking at the PnL and sharp ratio during that time period.  

Thus, just like in MNIST we label images with the number in pic, here we label each stock by its optimal weights. 

At first, we tried using `gurobipy` package for the optimization part. The objective function is maximizing $\alpha$ while minimizing $\beta$, for which: \\
<center>
$\alpha = R - R_f - \beta * (R_m - R_f)$ </br>
$\beta = \frac{Cov (R_m, R_e)}{Var(R_m)}$
</center>
$R$ is the return (in percentage) of our portfolio, $R_f$ is the risk free rate, $\beta$ measures the volatility, and $R_m$ represent the market return.   

After generating the optimal weights of each ten stocks, we'll be using L1 norm for the weights so each day's weights add up to 1.

#3.1: Optimization 
After trying optimization with 2 constraints in Milestone 1 (using https://towardsdatascience.com/scheduling-with-ease-cost-optimization-tutorial-for-python-c05a5910ee0d ), we decide that, due to the correlation between alpha and beta, there's no need to optimize both. Thus in Milestone 3 we created the optimized weights base on minimized $\beta$, A.K.A. reduce the risk of protfolio due to its correlation with market. Automatically, this should bring up $\alpha$. 

Our optimization tool is `scipy.optimize`. By setting the objective function to be minimizing beta, we want to find the weights which reduce the market risk. Here we are using 21 days as the tiemframe for calculating market risk, as each trading month has 21 days. We can try to increase the timeframe to have low market risk in a longer period, but the trade off will be runtime. 

The `weights.csv` is the optimal weights we generated. Warning: running the code below will take a loooooooong time. Recommand just upload the `weights.csv` file attached into colab.
"""

from scipy.optimize import minimize
import csv

i=0
while i <2522:
    data = np.array(pd.read_csv(stock_price_csv))
    stock = data[i:21+i,1:-1]
    market = data [i:21+i,-1]
    weights_init = np.array([[100]*stock.shape[1]]*stock.shape[0]).reshape((stock.shape[0], stock.shape[1])) #(2521,10)
    
    def f(weights):
        weights = weights.reshape((weights_init.shape[0],weights_init.shape[1]))
        var_rm = np.var(market)
        protfolio = []
        for day in range(weights.shape[0]):
            protfolio.append(np.dot(stock[day, :], weights[day,:]))
        protfolio = np.array(protfolio)
        mean_market = np.mean(market)
        mean_port = np.mean(market)
        sum_0 = 0
        
        for i in range(weights.shape[0]):
            sum_0+=abs(market[i]-mean_market)*abs(protfolio[i]-mean_port)
        cov_mar_prot = sum_0 / (market.shape[0] - 1) #covariance between market and protfolio 
        #print(cov_mat.shape)
        #print(cov.shape) #(2521, 2521)
        beta = cov_mar_prot / var_rm
        return beta 
    #print(f(weights_init))
    
    res = minimize(f, weights_init.flatten(), method='nelder-mead',options={'xatol': 1e-8, 'disp': True})
    if i==0:
        with open("weights.csv", 'w', newline='') as csvfile:
                fieldnames = ['stock0','1','2','3','4','5','6','7','8','9']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
                writer.writeheader()
                for stocks in res.x.reshape((21,10)):
                    sum_0 = np.sum(abs(i) for i in stocks)/100
                    writer.writerow({'stock0':stocks[0]/sum_0, '1': stocks[1]/sum_0, '2':stocks[2]/sum_0, 
                                     '3':stocks[3]/sum_0,'4' :stocks[4]/sum_0,'5' :stocks[5]/sum_0,'6' :stocks[6]/sum_0,'7' :stocks[7]/sum_0,'8' :stocks[8]/sum_0,'9' :stocks[9]/sum_0})
        csvfile.close()
    else:
        with open("weights.csv", 'a', newline='') as csvfile:
                writer = csv.writer(csvfile)
                for stocks in res.x.reshape((21,10)):
                    sum_0 = np.sum(abs(i) for i in stocks)/100

                    writer.writerow([stocks[0]/sum_0, stocks[1]/sum_0, stocks[2]/sum_0, 
                                     stocks[3]/sum_0,stocks[4]/sum_0, stocks[5]/sum_0, stocks[6]/sum_0, stocks[7]/sum_0, stocks[8]/sum_0, stocks[9]/sum_0])
        csvfile.close()
    i+=21


path = '/Users/emilygeng/MWTC2020_Smith/Case 3'
data = np.array(pd.read_csv(path + '/stock_prices_train.csv'))
stock = data[-21:,1:-1]
market = data [-21:,-1]
weights_init = np.array([[100]*stock.shape[1]]*stock.shape[0]).reshape((stock.shape[0], stock.shape[1])) #(2521,10)
    
def f(weights):
        weights = weights.reshape((weights_init.shape[0],weights_init.shape[1]))
        var_rm = np.var(market)
        protfolio = []
        for day in range(weights.shape[0]):
            protfolio.append(np.dot(stock[day, :], weights[day,:]))
        protfolio = np.array(protfolio)
        mean_market = np.mean(market)
        mean_port = np.mean(market)
        sum_1 = 0
        
        for i in range(weights.shape[0]):
            sum_1+=abs(market[i]-mean_market)*abs(protfolio[i]-mean_port)
        cov_mar_prot = sum_0 / (market.shape[0] - 1) #covariance between market and protfolio 
        #print(cov_mat.shape)
        #print(cov.shape) #(2521, 2521)
        beta = cov_mar_prot / var_rm
        return beta 
    #print(f(weights_init))
    
res = minimize(f, weights_init.flatten(), method='nelder-mead',options={'xatol': 1e-8, 'disp': True})
    
with open("weights.csv", 'a', newline='') as csvfile:
                writer = csv.writer(csvfile)
                stocks = np.mean(res.x.reshape((21,10)), axis = 0)
                sum_0 = np.sum(abs(i) for i in stocks)/100

                writer.writerow([stocks[0]/sum_0, stocks[1]/sum_0, stocks[2]/sum_0, 
                                     stocks[3]/sum_0,stocks[4]/sum_0, stocks[5]/sum_0, stocks[6]/sum_0, stocks[7]/sum_0, stocks[8]/sum_0, stocks[9]/sum_0])
csvfile.close()
'combine all files into one'
#path = '/Users/emilygeng/Downloads/USB/college/third_year/CSC591/weights_new'
#files = []
#for i in os.listdir(path):
#    if os.path.isfile(os.path.join(path,i)) and 'weights' in i:
#        files.append(i)
#print(files)

"""For dimentional concern, we need to add one more row in `weights.csv` in order to let it has the same dimension as `stock_price_csv`"""

weights = np.array(pd.read_csv('weights.csv')) #(2520,10)

weights.shape

"""#4: SNN
Deep SNNs are appeared to be a better solution. They’re able to work with event-based sensors and reach pseudo-simultaneous information processing [2]. Comparing to RNN, they are able to realize high speed localization and navigation [3]. Naturally deep SNNs involve temporal information, which make them more advanced in predicting time series data. The binary nature of them reduces energy usage and increases the model’s efficiency. Besides, past studies shows that SNNs tend to have better signal-to-noise ratio than ANNs [5].
The tricky part is the conversion between stock market information and spiking trains. The commonly used strategy is using Poisson process, which ignores the precise timing and focuses on average firing rates. For stock market specific, this could potential leads to prediction. Thus, instead of using the neuron fired first as the output class, we will be using a large population of neurons to reduce variance of input [3].

In Milestone 2, we will finish building SNN for trading algorithm and matching the weights with historical optimal weights.

#4.1: convert dataset
Here we created a subclass of torch dataset, based on UChicago Midwest Trading Competition's data (10 year stock data of 10 stocks). We created StockPriceDataset to replace the original MNIST dataset from hw.
"""

from torch.utils.data import Dataset
class StockPriceDataset(Dataset):
    """Stock Price dataset."""
    def __init__(self, transform=False):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        # print("=============")
        csv_file = stock_price_csv
        root_dir = "drive/My\ Drive/CSC691\ Project"
        self.stock_price = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform
        # print("-----------")

    def __len__(self):
        return len(self.stock_price)

    def __getitem__(self, idx): #idx from 0 to 11: date, stock [idx-1], market_index 
        day =  self.stock_price.iloc[idx,0] # the n-th day data for all stocks; image
        stocks = self.stock_price.iloc[idx,1:-1]
        stocks = np.array([stocks])
        stocks = stocks.astype('float').reshape(-1,10)
        # weights_daily = weights[idx,:]
        # weights_daily = np.array([weights_daily])
        # weights_daily = weights_daily.astype('float').reshape(-1,10)
        # print(stocks.shape, "============")
        # print(weights_daily.shape,"+++++++++++++")

        sample = {'stocks':stocks, 'day':day} #(1,10)
        if self.transform:
          sample['stocks'] = self.transform(sample['stocks'])
    # get train? pre-process? 
        return sample

"""#4.2: create wrapper
As we are creating the new dataset, we're also creating a new wrapper function for it. Here it will use day as label, and stock price for 10 stocks as image. We want to detect the pattern of stock price overtime with SNN.
"""

from typing import Optional, Dict
from bindsnet.encoding import Encoder, NullEncoder

class StockDatasetWrapper(StockPriceDataset):
        __doc__ = (
            """BindsNET torchvision dataset wrapper for:
        The core difference is the output of __getitem__ is no longer
        (image, label) rather a dictionary containing the image, label,
        and their encoded versions if encoders were provided.
            \n\n"""
            + str(StockPriceDataset)
            if StockPriceDataset.__doc__ is None
            else StockPriceDataset.__doc__
        )
        # print("2")


        def __init__(
            self,
            image_encoder: Optional[Encoder] = None,
            label_encoder: Optional[Encoder] = None,
            *args,
            **kwargs
        ):
            # language=rst
            """
            Constructor for the BindsNET torchvision dataset wrapper.
            For details on the dataset you're interested in visit
            https://pytorch.org/docs/stable/torchvision/datasets.html
            :param image_encoder: Spike encoder for use on the image
            :param label_encoder: Spike encoder for use on the label
            :param *args: Arguments for the original dataset
            :param **kwargs: Keyword arguments for the original dataset
            """
            # print("3")
            super().__init__(*args, **kwargs)

            self.args = args
            self.kwargs = kwargs
            # Allow the passthrough of None, but change to NullEncoder
            if image_encoder is None:
                image_encoder = NullEncoder()

            if label_encoder is None:
                label_encoder = NullEncoder()
            self.image_encoder = image_encoder
            self.label_encoder = label_encoder

        def __getitem__(self, ind: int) -> Dict[str, torch.Tensor]:
            # language=rst
            """
            Utilizes the ``torchvision.dataset`` parent class to grab the data, then
            encodes using the supplied encoders.
            :param int ind: Index to grab data at.
            :return: The relevant data and encoded data from the requested index.
            """
            item = super().__getitem__(ind)
            label = item['day']
            image = item['stocks'][0].int()
            # print( label, "=============label hahsh=============")
            output = {
                "image": image,
                "label": label,
                "encoded_image": self.image_encoder(image),
                "encoded_label": self.label_encoder(label),
            }

            return output

        def __len__(self):
            print("7")
            return super().__len__()

"""##4.3: Argument Parsing"""

seed = 0
n_neurons = 100
n_epochs = 1
n_test = 10000
n_workers = 0
exc = 22.5
inh = 120
theta_plus = 0.05
time = 100 
dt = 1.0
intensity = 128
progress_interval = 16
update_steps = 10
batch_size = 16 
train = True
gpu = False
plot=False
if not train:
    update_steps = n_test

n_sqrt = int(np.ceil(np.sqrt(n_neurons)))
start_intensity = intensity
update_interval = update_steps * batch_size

"""## 4.4: Configure for GPU Usage"""

# Sets up Gpu use
if gpu:
    torch.cuda.manual_seed_all(seed)
else:
    torch.manual_seed(seed)
    
# Determines number of workers to use
if n_workers == -1:
    n_workers = gpu * 4 * torch.cuda.device_count()

"""## 4.5: Creating the Network [copied from hw]

Here, we create a DiehlAndCook2015 network with an input size of 784 (28 x 28) and the passed/default arguments. The `exc` and `inh` values adjust the excitory and inhibitory hyperparameters of each neuron. The network, by default, is trained using a simple STDP learning rule in the library called `PostPre`. By default `PostPre` decreases the weights during pre-synaptic activity and increases the weights during post-synaptic activity.

For example, let us assume there are two neurons A and B. If neuron A fires before neuron B, then the weights between A and B are decreased, but if neuron A fires after neuron B, then the weights between A and B are increased.

The theory is that if one neuron fires after another neuron, they are related in that one neuron may be causing the other neuron to fire, so we increase the weights between those two neurons so that they are more likely to fire together. It is important to note that if implemented purely this way, then these two neurons would likely always only fire together. To combat this, we also introduce weight decay over time if the neurons aren't always firing together while training.
"""

# Build network.
network = DiehlAndCook2015(
    n_inpt=10,
    n_neurons=n_neurons,
    exc=exc,
    inh=inh,
    dt=dt,
    norm=78.4,
    theta_plus=theta_plus,
    inpt_shape=(1, 10),
)
# Directs network to GPU
if gpu:
    network.to("cuda")

"""##4.6: Loading the Data [from hw]

Spiking neural networks cannot directly use the intensity values in images. We must use an encoding to generate a spike train that we can feed into the SNN. A spike train is simply a dimensional tensor of 0s and 1s of a fixed length based on a hyperparameter (time).  When a value in the tensor is 1, that means at this index (time step), the neuron has spiked.

For example, we could have a single spike train:

`spike_train = [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0]`

We assume the indices of `spike_train` represents a time `t` and if `spike_train[t] = 1`, then the neuron has spiked at time `t`. The amount of time steps is the length of the spike train.

Now that we know what a spike train is, we must figure out how to convert our original data to a spike train. There are many types of encodings, but in this model, we use Poisson Encoding to convert each MNIST image to a spike train based on probabilities.

Our images are 28x28 with each `image[i][j]` representing a pixel intensity. We want to convert each pixel intensity to a spike train so that our input shape to the model will be `time`x28x28, where `time` is the hyperparameter for time steps.

Here is the general idea of how Poisson Encoding works per pixel intensity (Note: this is a visualization and not how it is directly implemented in the library):

```
spike_train = list()
poisson_probability(pixel_intensity) = probability #scalar value between 0 and 1
for i in range(time):
    if(random.random() < probability):
        spike_train[i] = 1
    else:
        spike_train[i] = 0
 ```
 
What we have done here is create a spike train that has a probability to spike based on a single pixel intensity. In effect, we have encoded a pixel intensity into a more sparse form which will be used for the SNN. Now, we just do this for every pixel in the image (28x28) and we have our input tensor for a single image of shape `time`x28x28.
"""

# Load data.
dataset = StockDatasetWrapper(
    PoissonEncoder(time=time, dt=dt),
    None,
    transform=transforms.Compose(
       [transforms.ToTensor()]#, transforms.Lambda(lambda x: x * intensity)]
    ),
)

"""##5. Setting up Accuracy Estimation

The fully trained network by nature should fire exactly one neuron in the final layer representing the input handwritten digit. However, because the ETH model is unsupervised, it is difficult to determine which neuron corresponds to what number, as there can be multiple neurons that represent a single digit and that the location of these neurons may be differ between training sessions. Finally, because there may be different neurons firing at different time steps, we must also decide on a method to select which neuron across time steps represents our input.

In the ETH model, we first train the model and then assign classes afterward. We average the responses of each neuron per class and select the class with the greatest average response as our prediction. The following code sets up the variables for later use in training.
"""

spike_record = torch.zeros(update_interval, time, n_neurons)

# Neuron assignments and spike proportions.
n_classes = 2510
assignments = -torch.ones(n_neurons)
proportions = torch.zeros(n_neurons, n_classes)
rates = torch.zeros(n_neurons, n_classes)

# Sequence of accuracy estimates.
accuracy = {"all": [], "proportion": []}
accuracy2 = {"all": [], "proportion": [], "21optimal":[]} #PnL
accuracy3 = {"all": [], "proportion": [], "21optimal":[]} #Sharp_ratio

"""##5.1: Visualizations

If we want to see how the network is running visually, we must set up voltage monitors for both the excitatory layer and inhibitory layer. This section of code is simply to help plot our data using `matplotlib`.
"""

# Voltage recording for excitatory and inhibitory layers.
exc_voltage_monitor = Monitor(network.layers["Ae"], ["v"], time=time)
inh_voltage_monitor = Monitor(network.layers["Ai"], ["v"], time=time)
network.add_monitor(exc_voltage_monitor, name="exc_voltage")
network.add_monitor(inh_voltage_monitor, name="inh_voltage")

# Set up monitors for spikes and voltages
spikes = {}
for layer in set(network.layers):
    spikes[layer] = Monitor(network.layers[layer], state_vars=["s"], time=time)
    network.add_monitor(spikes[layer], name="%s_spikes" % layer)

voltages = {}
for layer in set(network.layers) - {"X"}:
    voltages[layer] = Monitor(network.layers[layer], state_vars=["v"], time=time)
    network.add_monitor(voltages[layer], name="%s_voltages" % layer)

inpt_ims, inpt_axes = None, None
spike_ims, spike_axes = None, None
weights_im = None
assigns_im = None
perf_ax = None
voltage_axes, voltage_ims = None, None

"""##5.2: Training the Network

Finally, it is time to train the network. The code below has been commented to explain each step of the training.

We have two prediction methods `all_activity` and `proportion_weighting`. `all_activity` classifies the data with the highest average spiking activity over all neurons. `proportion_weighting` classifies the data with the highest average spiking acitivity over all neurons, but is also weighted by class-wise proportion.

##5.3: Define The New "Accuracy" 
We will judge the accuracy of the network by comparing the sharp ratio with optimal weights (generated earlier) as well as the sharo ratio with our predicted weights. The higher the sharp ratio the better our model works.
"""

# def get_sharp(all_stock_prices, daily_risk_free_rate, weights):
#     n_days,n_stocks = all_stock_prices.shape
#     daily_excess_return = [100]*n_days
#     Rp_list = np.array([0.0]*(n_days))
#     train_risk_free = np.array([0.00011731032720663]*(n_days))
#     """ hard code the first line. Delete later"""
#     all_stock_prices = np.concatenate((np.array([[100]*(n_stocks+2)]),all_stock_prices),axis=0)
#     daily_return = (all_stock_prices[1:,1:-1]-all_stock_prices[:-1,1:-1])/all_stock_prices[:-1,1:-1] # n-1
#     for day in range(n_days):
#         if day == 0:
#             weights[day] = np.array([[0.1]* n_stocks])
#             continue
#         bench_weight = np.array([0.1]*10)
#         train_risk_free[day] = daily_risk_free_rate[int(all_stock_prices[day,0])-1]
      
#         Rp_list[day] = np.dot(daily_return[day-1],weights[day,:].T)

#     daily_excess_return=Rp_list-train_risk_free

#     """ Need to duel with negative weights and nan"""
#     daily_excess_return = daily_excess_return[1:]
#     E = np.nanmean(daily_excess_return,axis = 0)
#     std_p = np.nanstd(daily_excess_return)
#     s_p = E / std_p * np.sqrt(252)
#     return s_p

stock_data_final = np.array(pd.read_csv(stock_price_csv)).reshape((2521,12))[1:,1:-1]
print(stock_data_final.shape)
bench_mark = np.array(pd.read_csv(stock_price_csv)).reshape((2521,12))[1:,-1]

# Commented out IPython magic to ensure Python compatibility.
# Commented out IPython magic to ensure Python compatibility.
# Train the network.

print("\nBegin training.\n")
start = t()

for epoch in range(n_epochs):
    labels = []

    if epoch % progress_interval == 0:
        print("Progress: %d / %d (%.4f seconds)" % (epoch, n_epochs, t() - start))
        start = t()

    # Create a dataloader to iterate and batch data
    dataloader = DataLoader(#torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=n_workers,
        pin_memory=gpu,
    )
    # print("loader")
    # print(dataloader.dataset, "ds")

    for step, batch in enumerate(dataloader):
        # print("success")
        # Get next input sample.
        # print(batch['encoded_image'].shape)
        inputs = {"X": batch["encoded_image"]}
        if gpu:
            inputs = {k: v.cuda() for k, v in inputs.items()}

        if step % update_steps == 0 and step > 0:
            # Convert the array of labels into a tensor
            label_tensor = torch.tensor(labels)
            # print("========= label_tensor =========", label_tensor, "\n", "========= label =========", labels)
            # print(label_tensor, "labels")


            # Get network predictions.
            all_activity_pred = all_activity(
                spikes=spike_record, assignments=assignments, n_labels=n_classes
            )
            proportion_pred = proportion_weighting(
                spikes=spike_record,
                assignments=assignments,
                proportions=proportions,
                n_labels=n_classes,
            )
            
            date_actual = label_tensor.long().numpy() #actual date
            date_pred_all = all_activity_pred.numpy() #all_method predicted date
            date_pred_proportional= proportion_pred.numpy() #proportioanl_method predicted date

            portfolio_actual =  []
            portfolio_all = []
            portfolio_proportion = []


            PnL_optimal = []
            PnL_snn_all = []
            PnL_snn_proportion = []

            sharp_optimal =[]
            sharp_snn_all = []
            sharp_snn_proportion = []

            for i in range(len(date_actual)):
              day = date_actual[i]
              day_all = date_pred_all[i]
              day_proportion = date_pred_proportional[i]

              portfolio_actual.append(np.dot(stock_data_final[day],weights[day]))
              portfolio_all.append(np.dot(stock_data_final[day], weights[day_all]))
              portfolio_proportion.append(np.dot(stock_data_final[day], weights[day_proportion]))

              PnL_optimal.append(np.dot(stock_data_final[day],weights[day]) - np.dot(stock_data_final[day-1],weights[day-1]))
              PnL_snn_all.append(np.dot(stock_data_final[day], weights[day_all]) - np.dot(stock_data_final[day-1], weights[day_all-1]))
              PnL_snn_proportion.append(np.dot(stock_data_final[day], weights[day_proportion]) - np.dot(stock_data_final[day-1], weights[day_proportion-1]))

              sharp_optimal.append(abs(np.dot(stock_data_final[day],weights[day]) - bench_mark[day]))
              sharp_snn_all.append(abs(np.dot(stock_data_final[day], weights[day_all]) - bench_mark[day]))
              sharp_snn_proportion.append(abs(np.dot(stock_data_final[day], weights[day_proportion]) - bench_mark[day]))
            # print("actual", sum(portfolio_actual),"all", sum(portfolio_all),"port", sum(portfolio_proportion))

            # Compute network accuracy according to available classification strategies.
            # ''''''''改改改''''''
            accuracy["all"].append(
                100*(1-abs(1-sum(portfolio_all) / sum(portfolio_actual)))
            )
            accuracy["proportion"].append(
                100*(1-abs(1-sum(portfolio_proportion) / sum(portfolio_actual)))
                
            )
            accuracy2["all"].append(
                100*(abs(sum(PnL_snn_all)) /(abs(sum(PnL_snn_all)) + abs(sum(PnL_optimal))+abs(sum(PnL_snn_proportion))))
            )
            accuracy2["proportion"].append(
                100*(abs(sum(PnL_snn_proportion)) /(abs(sum(PnL_snn_all)) + abs(sum(PnL_optimal))+abs(sum(PnL_snn_proportion))))
            )
            accuracy2["21optimal"].append(
                100*(abs(sum(PnL_optimal)) /(abs(sum(PnL_snn_all)) + abs(sum(PnL_optimal))+abs(sum(PnL_snn_proportion))))
            )

            accuracy3["all"].append(
                np.mean(sharp_snn_all) / np.std(sharp_snn_all)
            )
            accuracy3["proportion"].append(
                np.mean(sharp_snn_proportion) / np.std(sharp_snn_proportion)
            )
            accuracy3["21optimal"].append(
                np.mean(sharp_optimal) / np.std(sharp_optimal)
            )


            print(
                "\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)"
#                  % (
                    accuracy3["all"][-1],
                    np.mean(accuracy3["all"]),
                    np.max(accuracy3["all"]),
                )
            )
            print(
                "Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f (best)"
#                  % (
                    accuracy3["proportion"][-1],
                    np.mean(accuracy3["proportion"]),
                    np.max(accuracy3["proportion"]),
                )
            )
             
            print(
                "21 Day Optimal weighting accuracy: %.2f (last), %.2f (average), %.2f (best)\n"
#                  % (
                    accuracy3["21optimal"][-1],
                    np.mean(accuracy3["21optimal"]),
                    np.max(accuracy3["21optimal"]),
                )
            )

            # Assign labels to excitatory layer neurons.
            assignments, proportions, rates = assign_labels(
                spikes=spike_record,
                labels=label_tensor,
                n_labels=n_classes,
                rates=rates,
            )

            labels = []

        labels.extend(batch["label"].tolist())

        # Run the network on the input.
        network.run(inputs=inputs, time=time, input_time_dim=1)

        # Add to spikes recording.
        s = spikes["Ae"].get("s").permute((1, 0, 2))
        spike_record[
            (step * batch_size)
#              % update_interval : (step * batch_size % update_interval)
            + s.size(0)
        ] = s

        # Get voltage recording.
        exc_voltages = exc_voltage_monitor.get("v")
        inh_voltages = inh_voltage_monitor.get("v")

        # # Optionally plot various simulation information.
        # if plot:
        #     image = batch["image"][:, 0].view(28, 28)
        #     inpt = inputs["X"][:, 0].view(time, 784).sum(0).view(28, 28)
        #     input_exc_weights = network.connections[("X", "Ae")].w
        #     square_weights = get_square_weights(
        #         input_exc_weights.view(784, n_neurons), n_sqrt, 28
        #     )
        #     square_assignments = get_square_assignments(assignments, n_sqrt)
        #     spikes_ = {
        #         layer: spikes[layer].get("s")[:, 0].contiguous() for layer in spikes
        #     }
        #     voltages = {"Ae": exc_voltages, "Ai": inh_voltages}

        #     # inpt_axes, inpt_ims = plot_input(
        #     #     image, inpt, label=labels[step], axes=inpt_axes, ims=inpt_ims
        #     # )
        #     # spike_ims, spike_axes = plot_spikes(spikes_, ims=spike_ims, axes=spike_axes)
        #     weights_im = plot_weights(square_weights, im=weights_im)
        #     assigns_im = plot_assignments(square_assignments, im=assigns_im)
        #     perf_ax = plot_performance(accuracy, ax=perf_ax)
        #     # voltage_ims, voltage_axes = plot_voltages(
        #     #     voltages, ims=voltage_ims, axes=voltage_axes, plot_type="line"
        #     # )

        #     plt.pause(1e-8)

        network.reset_state_variables()  # Reset state variables.

print("Progress: %d / %d (%.4f seconds)" % (epoch + 1, n_epochs, t() - start))
print("Training complete.\n")

"""#5.4: Plot Learning Curve And Final Learned Weights (1 epoch)
Here we haven't implement the accuracy-->sharp ratio function yet. What we have here is matching the stock price with label (trading month), and see how far off we are. As the smaller the difference, the more accurate we are at predicting the trading month index, we shall see the plot in a reversed way: a sharp jump indicate a sudden decrease in accuracy. As we can see, `proportional` works better than `all` for the first half, and both works poorly in the second half. As we have 12*10 = 120 trading month, when we're off by 60 we're not doing better than random guess. 

I'm still working on implementing the new `all_activity` and `proportion_weighting` function. By clustering months with similar weights into one, instead of having adjacent date clustering together, the accuracy is expected to increase. I'm also working in implementing the sharp ratio function; with which, the plot will represent how good our predicted weights is at generating low risk profit comparing to "optimal weights" generated by `scipy.optimize`.
"""

plot_performance(accuracy, ax=perf_ax)

plot_performance(accuracy2, ax=perf_ax)

plot_performance(accuracy3, ax=perf_ax)